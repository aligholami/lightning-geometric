# @package _group_
optimizers:
  - name: 'default'
    _target_: torch.optim.Adam
    params:
      lr: 0.02
